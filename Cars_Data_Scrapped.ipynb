{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Daihatsu Car CSV"
      ],
      "metadata": {
        "id": "aPTL6uo7ogdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_daihatsu_car_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'daihatsu' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/daihatsu/12'\n",
        "num_pages = 101\n",
        "\n",
        "daihatsu_car_links = get_daihatsu_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in daihatsu_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllDaihatsuCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnbGffW52vG9",
        "outputId": "06f5c38a-a97c-4189-8e3a-8f336db80f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllDaihatsuCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nissan Cars CSV"
      ],
      "metadata": {
        "id": "M5dWFB5JL9dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_Nissan_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'nissan' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/nissan/26'\n",
        "num_pages = 57\n",
        "\n",
        "nissan_car_links = get_nissan_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in nissan_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllNissanCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ26AzdrL_w7",
        "outputId": "652cce12-9403-481e-df1a-7169af03ebd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllNissanCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audi Cars CSV\n"
      ],
      "metadata": {
        "id": "W1mVu6cHJZL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_audi_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'audi' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_audi/?page=10'\n",
        "num_pages = 10\n",
        "\n",
        "audi_car_links = get_audi_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in audi_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllAudiCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgi8iijtKUBv",
        "outputId": "7104bf4b-cfa5-4514-db55-947b08429c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllAudiCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BMW CAR CSV"
      ],
      "metadata": {
        "id": "tYdCHqCzLOYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_bmw_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'bmw' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_bmw/'\n",
        "num_pages = 7\n",
        "\n",
        "bmw_car_links = get_bmw_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in bmw_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllBmwCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwZgedzkLP8K",
        "outputId": "322a4dc1-ba10-479a-fc6b-62d3db673b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllBmwCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buick Car CSV"
      ],
      "metadata": {
        "id": "j9PngEreMZ2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_buick_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'buick' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_buick/'\n",
        "num_pages = 1\n",
        "\n",
        "buick_car_links = get_buick_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in buick_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllBuickCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpXF670AMbqv",
        "outputId": "446e7137-c5d6-4405-92ec-8b26cbc9d12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllBuickCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changan Car CSV"
      ],
      "metadata": {
        "id": "5qXjtjkfUtsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_changan_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'changan' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_changan/'\n",
        "num_pages = 22\n",
        "\n",
        "changan_car_links = get_changan_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in changan_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllChanganCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGNPSJpSU1Kl",
        "outputId": "da15c83f-527e-4bbd-a4e0-1f3aef9a3ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllChanganCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chevrolet Car CSV"
      ],
      "metadata": {
        "id": "1jrpp3HMV8PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_chevrolet_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'chevrolet' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_chevrolet/'\n",
        "num_pages = 4\n",
        "\n",
        "chevrolet_car_links = get_chevrolet_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in chevrolet_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllChevroletCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqcuREv6V-Mj",
        "outputId": "491634df-117b-469c-b778-edc42af0fb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllChevroletCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classic Cars CSV"
      ],
      "metadata": {
        "id": "6jEbS9eEXmdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_classiccars_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'classic-cars' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_classic-cars/'\n",
        "num_pages = 1\n",
        "\n",
        "classiccars_car_links = get_classiccars_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in classiccars_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllClassic-carsCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMhhAJoiXoFp",
        "outputId": "114537fc-b58e-4a1c-e45a-fd3221b098bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllClassic-carsCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daehan Cars CSV"
      ],
      "metadata": {
        "id": "jZrT37BwAMlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_daehan_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'daehan' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_daehan/'\n",
        "num_pages = 1\n",
        "\n",
        "daehan_car_links = get_daehan_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in daehan_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllDaehanCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HGln45LAs7o",
        "outputId": "dde09ad3-e616-4af4-ca80-24a8f3410bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllDaehanCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datsun Car CSV"
      ],
      "metadata": {
        "id": "T44ldL8cVAF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_datsun_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'datsun' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_datsun/'\n",
        "num_pages = 1\n",
        "\n",
        "datsun_car_links = get_datsun_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in datsun_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllDatsunCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns0f06fUVB3P",
        "outputId": "2ba7cff8-b386-4147-d517-280be313a620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllDatsunCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FAW Car CSV"
      ],
      "metadata": {
        "id": "iFQFJD-pV7hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_faw_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'faw' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_faw/'\n",
        "num_pages = 1\n",
        "\n",
        "faw_car_links = get_faw_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in faw_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllFawCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCByFEekV9P8",
        "outputId": "21dfcc32-940d-4626-cd65-dca8198839ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllFawCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ford Car CSV"
      ],
      "metadata": {
        "id": "IQW7RAr4XcCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_ford_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'ford' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_ford/'\n",
        "num_pages = 1\n",
        "\n",
        "ford_car_links = get_ford_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in ford_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllFordCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaxGBW25XeaV",
        "outputId": "1cc10382-564d-41d8-b12f-ff8ddfe4e5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllFordCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GUGO Car CSV"
      ],
      "metadata": {
        "id": "Fc7-NulVX83-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_gugo_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'gugo' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_gugo/'\n",
        "num_pages = 1\n",
        "\n",
        "gugo_car_links = get_gugo_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in gugo_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllGugoCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9HyKK5aX-iR",
        "outputId": "552400c7-569d-4168-af58-c9c166e8f33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllGugoCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HAVAL Car CSV"
      ],
      "metadata": {
        "id": "r0_pPiyGYin9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_haval_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'haval' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_haval/'\n",
        "num_pages = 5\n",
        "\n",
        "haval_car_links = get_haval_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in haval_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllHavalCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbvw7-X1Yl5U",
        "outputId": "3c7d8dd2-1bdf-4bb5-e136-699466991b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllHavalCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hummer Car CSV"
      ],
      "metadata": {
        "id": "q5WO4uroZIlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_hummer_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'hummer' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_hummer/'\n",
        "num_pages = 1\n",
        "\n",
        "hummer_car_links = get_hummer_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in hummer_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllHummerCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB-FuahiZKBv",
        "outputId": "b617a8e1-9a69-42aa-ae48-ed66273b995c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV File Created: AllHummerCarInfo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOYOTA CAR CSV"
      ],
      "metadata": {
        "id": "EF49CsaPA_wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_toyota_car_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'toyota' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/toyota/33'\n",
        "num_pages = 760\n",
        "\n",
        "toyota_car_links = get_toyota_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in toyota_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllToyotaCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "id": "MLRORBY1BB_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUZUKI CARS CSV"
      ],
      "metadata": {
        "id": "fPuFKzhiCvsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_suzuki_car_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'suzuki' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/suzuki/32'\n",
        "num_pages = 572\n",
        "\n",
        "suzuki_car_links = get_suzuki_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in suzuki_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllSuzukiCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "id": "saeCLOBoCvPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALL OTHER CARS CSV"
      ],
      "metadata": {
        "id": "EFT4hciNC6ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_other_car_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if any(brand.lower() in a.get('href', '').lower() for brand in ['Isuzu', 'JMC', 'Jaguar', 'KIA', 'Lexus', 'MINI', 'Mazda', 'Mitsubishi', 'Porsche', 'Prince', 'Proton', 'Range Rover', 'Rinco', 'Seres', 'Sogo', 'SsangYong', 'Subaru', 'Tesla', 'United', 'Volkswagen', 'Volvo', 'Willys', 'ZOTYE', 'Alfa Romeo', 'BAIC', 'Bentley', 'Cadillac', 'Chery', 'Chrysler', 'DFSK', 'Daewoo', 'Fiat', 'GMC', 'Genesis', 'Hino', 'Hyundai', 'JAC', 'JW Forland', 'Jeep', 'Land Rover', 'MG', 'Master', 'Mercedes Benz', 'Mushtaq', 'ORA', 'Peugeot', 'Power']) and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/search/-/mk_alfa-romeo/mk_baic/mk_bentley/mk_cadillac/mk_chery/mk_chrysler/mk_dfsk/mk_daewoo/mk_dodge/mk_fiat/mk_gmc/mk_genesis/mk_hino/mk_hyundai/mk_isuzu/mk_jac/mk_jmc/mk_jw-forland/mk_jaguar/mk_jeep/mk_kia/mk_land-rover/mk_lexus/mk_mg/mk_mini/mk_master/mk_mazda/mk_mercedes-benz/mk_mitsubishi/mk_mushtaq/mk_ora/mk_others/mk_peugeot/mk_porsche/mk_power/mk_prince/mk_proton/mk_range-rover/mk_rinco/mk_seres/mk_sogo/mk_ssangyong/mk_subaru/mk_tesla/mk_united/mk_volkswagen/mk_volvo/mk_willys/mk_zotye/'\n",
        "num_pages = 279\n",
        "\n",
        "other_car_links = get_other_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in other_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllOtherCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "id": "pQh6CyXUC937"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HONDA CARS CSV"
      ],
      "metadata": {
        "id": "2ecjJ5EXEgKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_car_info(url):\n",
        "    car_info = {\n",
        "        \"Name\": None,\n",
        "        \"Price\": None,\n",
        "        \"Year\": None,\n",
        "        \"Mileage\": None,\n",
        "        \"Fuel Type\": None,\n",
        "        \"Transmission\": None,\n",
        "        \"Registered In\": None,\n",
        "        \"Color\": None,\n",
        "        \"Assembly\": None,\n",
        "        \"Engine Capacity\": None,\n",
        "        \"Body Type\": None,\n",
        "        \"Features\": [],\n",
        "        \"Pictures\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            well_parent_h1 = soup.select_one('.well h1')\n",
        "            if well_parent_h1:\n",
        "                car_info[\"Name\"] = well_parent_h1.text.strip()\n",
        "\n",
        "            table_element = soup.find(class_='table table-bordered text-center table-engine-detail fs16')\n",
        "            if table_element:\n",
        "                td_tags = table_element.find_all('td')\n",
        "                if len(td_tags) >= 4:\n",
        "                    car_info[\"Year\"] = td_tags[0].text.strip()\n",
        "                    car_info[\"Mileage\"] = td_tags[1].text.strip()\n",
        "                    car_info[\"Fuel Type\"] = td_tags[2].text.strip()\n",
        "                    car_info[\"Transmission\"] = td_tags[3].text.strip()\n",
        "\n",
        "            price_box_element = soup.find(class_='price-box')\n",
        "            if price_box_element:\n",
        "                car_info[\"Price\"] = price_box_element.text.strip()\n",
        "\n",
        "            featured_list = soup.find(class_='list-unstyled ul-featured clearfix')\n",
        "            car_feature_list = soup.find(class_='list-unstyled car-feature-list nomargin')\n",
        "\n",
        "            if featured_list:\n",
        "                featured_li_elements = featured_list.find_all('li')\n",
        "                car_info[\"Registered In\"] = featured_li_elements[1].text.strip() if len(featured_li_elements) > 1 else None\n",
        "                car_info[\"Color\"] = featured_li_elements[3].text.strip() if len(featured_li_elements) > 3 else None\n",
        "                car_info[\"Assembly\"] = featured_li_elements[5].text.strip() if len(featured_li_elements) > 5 else None\n",
        "                car_info[\"Engine Capacity\"] = featured_li_elements[7].text.strip() if len(featured_li_elements) > 7 else None\n",
        "                car_info[\"Body Type\"] = featured_li_elements[9].text.strip() if len(featured_li_elements) > 9 else None\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for i, li_element in enumerate(featured_li_elements) if i not in [1, 3, 5, 7, 9]])\n",
        "\n",
        "            if car_feature_list:\n",
        "                feature_li_elements = car_feature_list.find_all('li')\n",
        "                car_info[\"Features\"].extend([li_element.text.strip() for li_element in feature_li_elements])\n",
        "\n",
        "            # Extract image URLs using a pattern in the src attribute\n",
        "            image_links = []\n",
        "\n",
        "            container = soup.find('ul', class_='gallery light-gallery list-unstyled cS-hidden')\n",
        "\n",
        "            if container:\n",
        "                images = container.find_all('li')\n",
        "\n",
        "                for image in images:\n",
        "                    if image.has_attr('data-src'):\n",
        "                        image_links.append(image['data-src'])\n",
        "\n",
        "            car_info[\"Pictures\"] = image_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping car info: {e}\")\n",
        "\n",
        "    return car_info\n",
        "\n",
        "def get_honda_car_links(base_url, num_pages):\n",
        "    all_links = []\n",
        "\n",
        "    try:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f'{base_url}?page={page}'\n",
        "\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                links = [a['href'] for a in soup.find_all('a', href=True) if 'honda' in a.get('href', '').lower() and 'for-sale-in' in a.get('href', '').lower()]\n",
        "\n",
        "                # Ensure that the obtained URLs are complete using urljoin\n",
        "                base_url_with_scheme = base_url if base_url.startswith(('http://', 'https://')) else 'https://' + base_url\n",
        "                links = [urljoin(base_url_with_scheme, link) for link in links]\n",
        "\n",
        "                all_links.extend(links)\n",
        "            else:\n",
        "                print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "# Example usage for the first 3 pages\n",
        "base_url = 'https://www.pakwheels.com/used-cars/honda/14'\n",
        "num_pages = 454\n",
        "\n",
        "honda_car_links = get_honda_car_links(base_url, num_pages)\n",
        "\n",
        "all_car_info = []\n",
        "\n",
        "for link in honda_car_links:\n",
        "    car_info = scrape_car_info(link)\n",
        "    all_car_info.append(car_info)\n",
        "\n",
        "# Write the information to a CSV file\n",
        "csv_file_path = 'AllHondaCarInfo.csv'\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header row\n",
        "    header = [\"Name\", \"Price\", \"Year\", \"Mileage\", \"Fuel Type\", \"Transmission\", \"Registered In\", \"Color\", \"Assembly\", \"Engine Capacity\", \"Body Type\", \"Features\", \"Pictures\"]\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write data rows\n",
        "    for car_info in all_car_info:\n",
        "        row = [\n",
        "            car_info[\"Name\"],\n",
        "            car_info[\"Price\"],\n",
        "            car_info[\"Year\"],\n",
        "            car_info[\"Mileage\"],\n",
        "            car_info[\"Fuel Type\"],\n",
        "            car_info[\"Transmission\"],\n",
        "            car_info[\"Registered In\"],\n",
        "            car_info[\"Color\"],\n",
        "            car_info[\"Assembly\"],\n",
        "            car_info[\"Engine Capacity\"],\n",
        "            car_info[\"Body Type\"],\n",
        "            ', '.join(car_info[\"Features\"]),  # Assuming features are a list\n",
        "            ', '.join(car_info[\"Pictures\"])   # Assuming pictures are a list\n",
        "        ]\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f'CSV File Created: {csv_file_path}')"
      ],
      "metadata": {
        "id": "7-pln8PCEb9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}